{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle DM2023 ISA5810 Lab2 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgoEbZzSYTpX"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "anfjcPSSYTpX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867535</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "2        0x29e452          train\n",
       "3        0x2b3819          train\n",
       "5        0x2a2acc          train\n",
       "6        0x2a8830          train\n",
       "7        0x20b21d          train\n",
       "...           ...            ...\n",
       "1867531  0x227e25          train\n",
       "1867532  0x293813          train\n",
       "1867533  0x1e1a7e          train\n",
       "1867534  0x2156a5          train\n",
       "1867535  0x2bb9d2          train\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data_identification.csv\n",
    "df = pd.read_csv(\"./data_identification.csv\",\n",
    "                sep=\",\", header=None,names=[\"tweet_id\", \"identification\"])\n",
    "\n",
    "# Obtain train_id and test_id through identification\n",
    "train_id = df[df[\"identification\"] == \"train\"]\n",
    "test_id = df[df[\"identification\"] == \"test\"]\n",
    "train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ËÆÄÂèñ tweets_DM.json\n",
    "with open(\"./tweets_DM.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tweets_data = [json.loads(line)[\"_source\"][\"tweet\"] for line in f]\n",
    "\n",
    "# ÂèñÂæóÊé®ÊñáÊñáÂ≠óÂàóË°®\n",
    "tweet_texts = [tweet[\"text\"] for tweet in tweets_data]\n",
    "\n",
    "# Âà™Èô§ <LH> Ê®ôÁ±§\n",
    "cleaned_texts = [text.replace(\"<LH>\", \"\") for text in tweet_texts]\n",
    "\n",
    "# Êõ¥Êñ∞Êé®ÊñáÊñáÂ≠ó\n",
    "for i in range(len(tweets_data)):\n",
    "    tweets_data[i][\"text\"] = cleaned_texts[i]\n",
    "\n",
    "# Â∞áÊõ¥Êñ∞ÂæåÁöÑË≥áÊñôËΩâÊèõÂõû JSON Â≠ó‰∏≤\n",
    "updated_json_str = json.dumps(tweets_data, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_df:  (1455563, 3)\n",
      "   tweet_id                                               text       emotion\n",
      "0  0x29e452  Huge Respectüñí @JohnnyVegasReal talking about l...           joy\n",
      "1  0x2b3819  Yoooo we hit all our monthly goals with the ne...           joy\n",
      "2  0x2a2acc  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...         trust\n",
      "3  0x2a8830  Come join @ambushman27 on #PUBG while he striv...           joy\n",
      "4  0x20b21d  @fanshixieen2014 Blessings!My #strength little...  anticipation\n"
     ]
    }
   ],
   "source": [
    "# Â∞á JSON Â≠ó‰∏≤ËΩâÊèõÊàê DataFrame\n",
    "tweets_df = pd.json_normalize(tweets_data)[[\"tweet_id\", \"text\"]]\n",
    "\n",
    "# ËÆÄÂèñ emotion.csv\n",
    "emotion_df = pd.read_csv(\"./emotion.csv\", sep=\",\", header=None, names=[\"tweet_id\", \"emotion\"])\n",
    "\n",
    "# Âêà‰ΩµË≥áÊñôÈõÜ\n",
    "merged_train_df = pd.merge(train_id, tweets_df, on=\"tweet_id\", how=\"left\")\n",
    "train_df = pd.merge(merged_train_df, emotion_df, on=\"tweet_id\", how=\"left\")\n",
    "test_df = pd.merge(test_id, emotion_df, on=\"tweet_id\", how=\"left\")\n",
    "\n",
    "# ÁßªÈô§ 'identification' Âàó\n",
    "train_df = train_df.drop(columns=[\"identification\"])\n",
    "test_df = test_df.drop(columns=[\"identification\"])\n",
    "\n",
    "# È°ØÁ§∫ÁµêÊûú\n",
    "print(\"Shape of train_df: \", train_df.shape)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HBHwcL8sYTpX"
   },
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w_cDUwCYTpX",
    "outputId": "3582ac44-1f5f-4cb2-b833-d477f152461a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (1455563, 3)\n",
      "Shape of Testing df:  (411972, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", train_df.shape)\n",
    "print(\"Shape of Testing df: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hr8aKhlYTpo"
   },
   "source": [
    "---\n",
    "### Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm6GF2VvYTpo"
   },
   "source": [
    "We will save our data in Pickle format. The pickle module implements binary protocols for serializing and de-serializing a Python object structure.   \n",
    "  \n",
    "Some advantages for using pickle structure:  \n",
    "* Because it stores the attribute type, it's more convenient for cross-platform use.  \n",
    "* When your data is huge, it could use less space to store also consume less loading time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dZzepBdpYTpo"
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H5uO-kOUYTpo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "# Â∞áÊÉÖÁ∑íÊ®ôÁ±§ËΩâÊèõÊàêÊï∏Â≠ó\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"emotion_label\"] = label_encoder.fit_transform(train_df[\"emotion\"])\n",
    "\n",
    "# Â∞áÊñáÊú¨ÈÄ≤Ë°åÂàÜË©û\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])\n",
    "sequences = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "\n",
    "# Â∞áÂ∫èÂàóÂ°´ÂÖÖÊàêÁõ∏ÂêåÁöÑÈï∑Â∫¶\n",
    "max_sequence_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# ÂàáÂâ≤Ë®ìÁ∑¥ÈõÜÂíåÈ©óË≠âÈõÜ\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, train_df[\"emotion_label\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 12:34:59.656071: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-27 12:34:59.697578: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-27 12:34:59.697979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 12:35:00.314641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-27 12:35:55.504969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 12:35:55.507189: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 12:35:55.859809: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 465780000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36389/36390 [============================>.] - ETA: 0s - loss: 1.2503 - accuracy: 0.5475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 13:07:27.880423: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 116445200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36390/36390 [==============================] - 1977s 54ms/step - loss: 1.2503 - accuracy: 0.5475 - val_loss: 1.1784 - val_accuracy: 0.5733\n",
      "Epoch 2/10\n",
      "36390/36390 [==============================] - 1957s 54ms/step - loss: 1.1633 - accuracy: 0.5795 - val_loss: 1.1580 - val_accuracy: 0.5808\n",
      "Epoch 3/10\n",
      "36390/36390 [==============================] - 1953s 54ms/step - loss: 1.1369 - accuracy: 0.5890 - val_loss: 1.1458 - val_accuracy: 0.5853\n",
      "Epoch 4/10\n",
      "36390/36390 [==============================] - 1956s 54ms/step - loss: 1.1208 - accuracy: 0.5946 - val_loss: 1.1422 - val_accuracy: 0.5869\n",
      "Epoch 5/10\n",
      "36390/36390 [==============================] - 1949s 54ms/step - loss: 1.1104 - accuracy: 0.5985 - val_loss: 1.1393 - val_accuracy: 0.5890\n",
      "Epoch 6/10\n",
      "36390/36390 [==============================] - 1953s 54ms/step - loss: 1.1029 - accuracy: 0.6014 - val_loss: 1.1434 - val_accuracy: 0.5881\n",
      "Epoch 7/10\n",
      "36390/36390 [==============================] - 1950s 54ms/step - loss: 1.0969 - accuracy: 0.6034 - val_loss: 1.1443 - val_accuracy: 0.5873\n",
      "Epoch 8/10\n",
      "36390/36390 [==============================] - 1947s 53ms/step - loss: 1.0927 - accuracy: 0.6053 - val_loss: 1.1435 - val_accuracy: 0.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 16:56:44.824483: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 164788800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12875/12875 [==============================] - 124s 10ms/step\n",
      "              id       emotion\n",
      "113935  0x2ef354           joy\n",
      "234392  0x24b811         trust\n",
      "123482  0x36c277           joy\n",
      "395126  0x2cbaa6  anticipation\n",
      "343251  0x2dd496       disgust\n"
     ]
    }
   ],
   "source": [
    "# Âª∫Á´ãÊ®°Âûã\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(8, activation=\"softmax\"))\n",
    "\n",
    "# Á∑®Ë≠ØÊ®°Âûã\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# ÂÆöÁæ©ÊèêÂâçÂÅúÊ≠¢Ê¢ù‰ª∂\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "# Ë®ìÁ∑¥Ê®°Âûã\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# ‰ΩøÁî®Ê∏¨Ë©¶Ë≥áÊñôÈÄ≤Ë°åÈ†êÊ∏¨\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"text\"])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "predictions = model.predict(padded_test_sequences)\n",
    "\n",
    "# Â∞áÈ†êÊ∏¨ÁöÑÊÉÖÁ∑íÊ®ôÁ±§ËΩâÊèõÂõûÊñáÂ≠ó\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Âª∫Á´ãÊ∏¨Ë©¶ÁµêÊûúÁöÑDataFrame\n",
    "test_results_df = pd.DataFrame({\"id\": test_df[\"tweet_id\"], \"emotion\": predicted_labels})\n",
    "\n",
    "# È°ØÁ§∫ÁµêÊûú\n",
    "print(test_results_df.head())\n",
    "\n",
    "# ÂÑ≤Â≠òÊ∏¨Ë©¶ÁµêÊûú\n",
    "test_results_df.to_csv(\"test_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Testing df:  (411972, 2)\n"
     ]
    }
   ],
   "source": [
    "test_results = pd.read_csv(\"./test_results.csv\", sep=\",\")\n",
    "print(\"Shape of Testing df: \", test_results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fF1woa8YTp5"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4e5eiVLOYTp5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 594.85,
   "position": {
    "height": "40px",
    "left": "723px",
    "right": "20px",
    "top": "80px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
